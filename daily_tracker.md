## Course 1

| Date        | Videos Done           | On Track  | Comments |
| ------------- |:-------------:| -----:|-----------------|
| 02/09/18  | Week 1 Done | Yes | Will See Hinton's Interview after this course |
| 04/09/18  | Week 2 In Progress LR using NN is done | Yes | Revise Graphs and Derivatives (Log and Exponential Functions |
| 05/09/18  | Week 2 Assignment Done and Completed | Yes | np.reshape() is a problem, Function of optimizer is also to be discussed |
| 06/09/18  | Week 3 Started Till Activation Functions | Yes | Why we use non linear Activation Function is Done and Understood! Explore more about various Activation Functions|
| 10/09/18  | Week 3 Done | Yes | Revise Derivatives of all. Don't cram formulas derive formulas later on or in interviews|
| 11/09/18  | Week 3 Completed with Programming Exercise | Yes | np.squeeze(), also sigmoid function does not computes the final output, log loss and cross entropy loss difference|
| 12/09/18  | Week 4 Videos done with Assingment | Yes | Backprop equations is still a gray area, hopefully this assignment will clear it|
| 13/09/18  | Week 4 Assingments are done | Yes |  Had hard time visualizing backpropagation but finally got the course certificate and knowledge, moving on to next|

## Course 2

| Date        | Videos Done           | On Track  | Comments |
| ------------- |:-------------:| -----:|-----------------|
| 14/09/18  | Week 1 Done till Regularization | Yes |L1 Regularization, L2 Regularization and (np.dot, np.muliply and np.matmul difference |
| 18/09/18  | Week 1 Done Complete | Yes |L1 Regularization, L2 Regularization are still points that need more attention, Gradient Checking is theoretical, will try to dive in further |
| 19/09/18  | Week 2 Done Till GD with Momentum | Yes | Exponential Weighted Averages a beautiful Algo for averaging out and then use GD with momentum (which means exponential averaging out the gradients)|
| 22/09/18  | Completed Tensorflow | Yes | Revise Tensorflow implementation. On to course 3|


## Course 3

| Date        | Videos Done           | On Track  | Comments |
| ------------- |:-------------:| -----:|-----------------|
| 16/10/2018 | Week 1 Completed | Yes | Very Basic Chapters and comments but interesting exercise to work on. |
| 18/10/2018 | Week 2 Completed | Yes | Got the Certificate but most importantly got some theoretical concepts which I would have never known. For example: multi class learning |



## Course 4

| Date        | Videos Done           | On Track  | Comments |
| ------------- |:-------------:| -----:|-----------------|
| 21/10/2018 | Week 1 In Progress | Yes | Beautifully explained the concept of Convolutions over Volumes. Also the use of Stride and Padding |
| 22/10/2018 | Week 1 Completed | Yes | Completed the week but still need to find out how backpropagation updates the weights of the filters. It is there is Exercise 1 optional assignment. Sad that there is no video to support that |
| 23/10/2018 | Week 2 1st half Videos Completed | Yes | Beautiful concepts explaining the Architectures and 1X1 Convolutions and ResNets |
| 24/10/2018 | Week 2 Videos Completed | Yes | Videos were too long. Explains Inception Networks where we use every type of convolution and some good practices for competitions, ensembling and test time cropping |
| 25/10/2018 | Week 2 Completed | Yes | Completed Assigments. There were two parts of Resnets in the Assignment: Identity-in which the size while addition is the same, Convolutional: where we perform the required convolution before adding to perform the addition |
| 29/10/2018 | Week 3 Videos Completed | Yes | Concepts of YOLO were awesome, please revise them again |
| 30/10/2018 | Week 3 Videos Complete | Yes | YOLO Assignment went over the head did not undestood anything |
| 05/11/2018 | Week 4 Face Recognition done | Yes | Very Interesting Presentation and triplet loss looks good on paper but have to implement it as soon as possible |
| 07/11/2018 | Week 4 Videos Neural Style Transfer Done | Yes | Style loss and content loss. Content loss difference between activations of a single layer and style loss is the difference of correlation between different channels of the image(gram matrix) this is done on mulitple layers |
| 08/11/2018 | Week 4 Complete and Got the Certificate | Yes | Theory looks good on paper but actual implementation will be the key to success|


## Course 5

| Date        | Videos Done           | On Track  | Comments |
| ------------- |:-------------:| -----:|-----------------|
| 15/11/2018 | Week 1 In Progress | Yes | What are RNN's, Applications of Sequence Models |
| 16/11/2018 | Week 1 In Progress | Yes | Different RNN architectures, Backpropagation through time |
